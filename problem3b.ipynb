{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###You mask the word \"charge\" and predict replacements using BERT.\n",
    "###Convert these replacements into binary vectors based on unique words.\n",
    "###Use KMeans clustering to group similar passages into clusters based on the replacement words\n",
    "###\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Step 1: Load BERT Model and Tokenizer\n",
    "checkpoint = 'bert-base-uncased'\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Step 2: Function to get top 20 replacements for \"[MASK]\" from the file on Canvas \n",
    "## determine words that the model rates most highly  as replacements of <mask>:\n",
    "def get_top_replacements(text, top_n=20):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)  # Ensure truncation for long texts\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(**inputs).logits\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_tokens = torch.topk(mask_token_logits, top_n, dim=1).indices[0].tolist()\n",
    "    return [tokenizer.decode([token]) for token in top_tokens]\n",
    "\n",
    "# Step 3: Load passages from file and mask the word \"charge\"\n",
    "file_path = 'problem3_data.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    passages = file.readlines()\n",
    "\n",
    "masked_passages = [p.replace('charge', '[MASK]') for p in passages]\n",
    "\n",
    "# Step 4: Get top 20 replacements for \"charge\" in each passage\n",
    "all_replacements = []\n",
    "for passage in masked_passages:\n",
    "    replacements = get_top_replacements(passage)\n",
    "    if replacements:  # Only append if replacements were found\n",
    "        all_replacements.append(replacements)\n",
    "\n",
    "# Step 5: Vectorize the substitutes for clustering\n",
    "# Create a set of all unique substitutes across all passages\n",
    "unique_substitutes = list(set([word for sublist in all_replacements for word in sublist]))\n",
    "\n",
    "# Function to convert each passage's substitutes into a binary vector\n",
    "def get_binary_vector(replacements, unique_substitutes):\n",
    "    return [1 if word in replacements else 0 for word in unique_substitutes]\n",
    "\n",
    "# Create the binary matrix (one row per passage, one column per unique substitute)\n",
    "binary_vectors = [get_binary_vector(replacements, unique_substitutes) for replacements in all_replacements]\n",
    "\n",
    "# Step 6: Cluster the passages using K-Means\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(binary_vectors)\n",
    "\n",
    "# Step 7: Output the clusters and the respective passages\n",
    "clustered_passages = {}\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    if label not in clustered_passages:\n",
    "        clustered_passages[label] = []\n",
    "    clustered_passages[label].append(passages[i])\n",
    "\n",
    "print(clustered_passages)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
