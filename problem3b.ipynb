{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m     12\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Step 1: Load BERT Model and Tokenizer\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "###\n",
    "###You mask the word \"charge\" and predict replacements using BERT.\n",
    "###Convert these replacements into binary vectors based on unique words.\n",
    "###Use KMeans clustering to group similar passages into clusters based on the replacement words\n",
    "###\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Step 1: Load BERT Model and Tokenizer\n",
    "checkpoint = 'bert-base-uncased'\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Step 2: Function to get top 20 replacements for \"[MASK]\" from the file on Canvas \n",
    "## determine words that the model rates most highly  as replacements of <mask>:\n",
    "def get_top_replacements(text, top_n=20):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)  # Ensure truncation for long texts\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(**inputs).logits\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_tokens = torch.topk(mask_token_logits, top_n, dim=1).indices[0].tolist()\n",
    "    return [tokenizer.decode([token]) for token in top_tokens]\n",
    "\n",
    "# Step 3: Load passages from file and mask the word \"charge\"\n",
    "file_path = 'problem3_data.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    passages = file.readlines()\n",
    "\n",
    "masked_passages = [p.replace('charge', '[MASK]') for p in passages]\n",
    "\n",
    "# Step 4: Get top 20 replacements for \"charge\" in each passage\n",
    "all_replacements = []\n",
    "for passage in masked_passages:\n",
    "    replacements = get_top_replacements(passage)\n",
    "    if replacements:  # Only append if replacements were found\n",
    "        all_replacements.append(replacements)\n",
    "\n",
    "# Step 5: Vectorize the substitutes for clustering\n",
    "# Create a set of all unique substitutes across all passages\n",
    "unique_substitutes = list(set([word for sublist in all_replacements for word in sublist]))\n",
    "\n",
    "# Function to convert each passage's substitutes into a binary vector\n",
    "def get_binary_vector(replacements, unique_substitutes):\n",
    "    return [1 if word in replacements else 0 for word in unique_substitutes]\n",
    "\n",
    "# Create the binary matrix (one row per passage, one column per unique substitute)\n",
    "binary_vectors = [get_binary_vector(replacements, unique_substitutes) for replacements in all_replacements]\n",
    "\n",
    "# Step 6: Cluster the passages using K-Means\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(binary_vectors)\n",
    "\n",
    "# Step 7: Output the clusters and the respective passages\n",
    "clustered_passages = {}\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    if label not in clustered_passages:\n",
    "        clustered_passages[label] = []\n",
    "    clustered_passages[label].append(passages[i])\n",
    "\n",
    "print(clustered_passages)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
